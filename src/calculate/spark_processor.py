import os
import psycopg2
import datetime
import subprocess
from pyspark import SparkContext, SQLContext, SparkConf
from pyspark.sql.functions import col, datediff, to_date, date_add, coalesce


class SparkProcessor:

    def __init__(self, aws_id, aws_key, host, dbname, user, password):
        sc_conf = SparkConf()
        sc_conf.setAppName('ghalarge_cluster_6G')
        sc_conf.setMaster(
            'spark://ec2-52-45-53-97.compute-1.amazonaws.com:7077')
        sc_conf.set('spark.executor.memory', '6g')
        sc_conf.set('spark.submit.deployMode', 'cluster')
        sc_conf.set('spark.jars', '../../lib/postgresql-42.2.5.jar')

        sc = SparkContext(conf=sc_conf)
        hadoop_conf = sc._jsc.hadoopConfiguration()
        hadoop_conf.set("fs.s3n.awsAccessKeyId", aws_id)
        hadoop_conf.set("fs.s3n.awsSecretAccessKey", aws_key)
        self.sqlContext = SQLContext(sc)

        self.default_start_datetime = datetime.datetime(2011, 2, 11, 0)
        self.seven_days = {}
        self.mode = 'overwrite'

        # Connect to DB
        print('Connecting to DB...')
        self.conn = psycopg2.connect(host=host, database=dbname, user=user, password=password)
        self.postgres_url = 'jdbc:postgresql://%s/%s' % (host, dbname)
        self.properties = {'user': user, 'password': password, 'driver': "org.postgresql.Driver"}
        print('Connected.')

    def df_jdbc_write_to_db(self, df, table_name, mode='append'):
        """
        Write dataframe directlly to postgres using jdbc
        """
        df.write \
            .format("jdbc") \
            .mode(mode) \
            .option("driver", self.properties['driver']) \
            .option("url", self.postgres_url) \
            .option("dbtable", table_name) \
            .option("user", self.properties['user']) \
            .option("password", self.properties['password']) \
            .save()

    def df_psycopg2_write_to_db(self, insert_sql, insert_param):
        """
        If wheather_create_table is True, execute create_table_sql.
        Eexecute insert_sql with insert_paramself.
        Append backup_sentence to self.backup_file.
        """
        cur = self.conn.cursor()
        try:
            cur.execute(insert_sql, insert_param)
        except psycopg2.IntegrityError as ie:
            print("Diplicate key.")

        self.conn.commit()
        cur.close()

    def read_all_to_df(self, bucket_name, path):
        """
        Given a bucket name read all file on that bucket to a df
        """
        print(path)
        df = self.sqlContext.read.json(path)
        return df

    def read_files_to_df(self, urls):
        """
        Given a list of s3 urls, return a dataframe generated by sqlContext
        """
        df = self.sqlContext.read.json(urls)
        return df

    def process_history_df(self, df):
        """
        Process function for history data, generate result dataframe
        that contains date, number of create events and
        growth rate of a day compare to last week
        """
        # There are two versions of API for CreateEvent of repository:
        # - One is        col("payload")['object'] == 'repository'
        # - Another is    col("payload")['ref_type'] == 'repository'
        # try:
        df_columns = df.columns
        df_first_record = df.first()
        keyword = 'object' if 'object' in df_first_record['payload'] else 'ref_type'

        num_create_events_df = \
            df \
            .filter(col('payload')[keyword] == 'repository') \
            .filter((col('type') == 'CreateEvent') | (col('type') == 'Event'))

        # count the number of create events happened in one day (group by date)
        num_create_events_by_date_df = num_create_events_df.groupby(to_date(df.created_at).alias('date_created_at')).count()

        # calculate the grawth rate of that day compare to last week
        # dulicated two dataframes, for each day in the first dataframe
        # find the number fo create events in the second dataframe
        # of a day that is 7 days before the day in the first dataframe
        # [df1] 2015-01-07 -> [df2] 2015-01-01 (7 days)
        num_create_events_by_date_df_1 = num_create_events_by_date_df.alias('num_create_events_by_date_df_1')

        num_create_events_by_date_df_1 = \
            num_create_events_by_date_df_1 \
            .select(
                col('date_created_at').alias('date_created_at_1'),
                col('count').alias('count_1'))

        num_create_events_by_date_df_2 = num_create_events_by_date_df.alias('num_create_events_by_date_df_2')

        num_create_events_by_date_df_2 = \
            num_create_events_by_date_df_2 \
            .select(
                col('date_created_at').alias('date_created_at_2'),
                col('count').alias('count_2'))

        joined_num_create_events_df = \
            num_create_events_by_date_df_1 \
            .withColumn(
                'last_week_date_created_at',
                date_add(num_create_events_by_date_df_1.date_created_at_1, -7)) \
            .join(
                num_create_events_by_date_df_2,
                col('last_week_date_created_at')
                == col('date_created_at_2'),
                how='left_outer')

        joined_num_create_events_df = joined_num_create_events_df.withColumn(
            'count_2', coalesce('count_2', 'count_1'))

        num_create_events_with_growth_rate_df = \
            joined_num_create_events_df \
            .withColumn(
                'weekly_increase_rate',
                ((joined_num_create_events_df.count_1 - joined_num_create_events_df.count_2) / joined_num_create_events_df.count_2)
            ) \
            .select(
                'date_created_at_1',
                'count_1',
                'weekly_increase_rate')

        num_create_events_with_growth_rate_df.show()

        return num_create_events_with_growth_rate_df

    def process_present_df(self, present_df, table_name):
        """
        Process function for generate result dataframe that contains date,
        number of create events and growth rate of a day compare to last week
        """
        df_columns = present_df.columns
        df_first_record = present_df.first()
        keyword = 'object' if 'object' in df_first_record['payload'] else 'ref_type'

        num_create_events_df = \
            present_df \
            .filter(col('payload')[keyword] == 'repository') \
            .filter((col('type') == 'CreateEvent') | (col('type') == 'Event'))

        num_create_events_by_date_df = \
            num_create_events_df \
            .groupby(to_date(present_df.created_at).alias('date_created_at')) \
            .count()

        return num_create_events_by_date_df.withColumn(
            'weekly_increase_rate',
            self.get_num_created_repo(table_name, date_add(num_create_events_by_date_df.date_created_at, -7))
        )

    def read_all_to_list(self, spark_recent_todo_files):
        """
        Read a file, each line is a url to s3n://bucket/file.json
        => ["s3n://bucket/file.json", "s3n://bucket/file1.json", ...]
        """
        if not os.path.isfile(spark_recent_todo_files):
            open(spark_recent_todo_files, 'w').close()
        result = []
        with open(spark_recent_todo_files) as fin:
            for url in fin:
                result.append(url.strip())
        return result

    def generate_todo_urls(self, spark_recent_todo_files, table_name, bucket_name, start_date=datetime.datetime.utcnow()):
        """
        Given start_date and table name and bucket name, generate aws s3 urls
        """
        one_day = datetime.timedelta(days=1)
        utc_now = start_date
        with open(spark_recent_todo_files, 'w') as fout:
            last_date = None
            while not last_date:
                # get last processed date
                cur = self.conn.cursor()
                cur.execute(
                    'select exists(select 1 from ' + table_name + ' where date_created_at_1 = %s)',
                    (start_date,))
                last_date = cur.fetchone()[0]
                start_date -= one_day
            while start_date < utc_now:
                start_date += one_day
                fout.write('s3n://' + bucket_name + '/' + start_date.strftime("%Y-%m-%d*") + '\n')
